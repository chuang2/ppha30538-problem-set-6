---
title: "Problem Set 6 - Waze Shiny Dashboard"
author: "Peter Ganong, Maggie Shi, and Andre Oviedo"
date: today
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---
1. **ps6:** Due Sat 23rd at 5:00PM Central. Worth 100 points (80 points from questions, 10 points for correct submission and 10 points for code style) + 10 extra credit. 

We use (`*`) to indicate a problem that we think might be time consuming. 

# Steps to submit (10 points on PS6) {-}

1. "This submission is my work alone and complies with the 30538 integrity
policy." Add your initials to indicate your agreement: \*\*\_\_\*\*
2. "I have uploaded the names of anyone I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  \*\*\_\_\*\* (2 point)
3. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*

4. Before starting the problem set, make sure to read and agree to the terms of data usage for the Waze data [here](https://canvas.uchicago.edu/courses/59054/quizzes/130617).

5. Knit your `ps6.qmd` as a pdf document and name it `ps6.pdf`.
6. Push your `ps6.qmd`, `ps6.pdf`, `requirements.txt`, and all created folders (we will create three Shiny apps so you will have at least three additional folders) to your Github repo (5 points). It is fine to use Github Desktop.
7. Submit `ps6.pdf` and also link your Github repo via Gradescope (5 points)
8. Tag your submission in Gradescope. For the Code Style part (10 points) please tag the whole correspondingsection for the code style rubric.

*Notes: see the [Quarto documentation (link)](https://quarto.org/docs/authoring/figures.html) for directions on inserting images into your knitted document.*

*IMPORTANT: For the App portion of the PS, in case you can not arrive to the expected functional dashboard we will need to take a look at your `app.py` file. You can use the following code chunk template to "import" and print the content of that file. Please, don't forget to also tag the corresponding code chunk as part of your submission!*

```{python}
#| echo: true
#| eval: false

def print_file_contents(file_path):
    """Print contents of a file."""
    try:
        with open(file_path, 'r') as f:
            content = f.read()
            print("```python")
            print(content)
            print("```")
    except FileNotFoundError:
        print("```python")
        print(f"Error: File '{file_path}' not found")
        print("```")
    except Exception as e:
        print("```python") 
        print(f"Error reading file: {e}")
        print("```")

print_file_contents("./top_alerts_map_byhour/app.py") # Change accordingly
```

```{python} 
#| echo: false

# Import required packages.
import pandas as pd
import altair as alt 
import pandas as pd
from datetime import date
import numpy as np
alt.data_transformers.disable_max_rows() 

import json
```

# Background {-}

## Data Download and Exploration (20 points){-} 

1. 

```{python}
import zipfile

zipfile_path = "/Users/charleshuang/Documents/GitHub/real_merge_example/ppha30538-problem-set-6/waze_data.zip"

extract_to = "/Users/charleshuang/Documents/GitHub/real_merge_example/ppha30538-problem-set-6"

with zipfile.ZipFile(zipfile_path, 'r') as zip_ref:
    zip_ref.extractall(extract_to)

print(f"Files have been extracted to {extract_to}")


```

Loading waze sample data into a df:

```{python}
wz_sample = pd.read_csv("waze_data_sample.csv", index_col=None)

wz_sample.head()
wz_sample = wz_sample.drop(columns=["Unnamed: 0"])

#Variable names:

wz_vars = wz_sample.columns
print(wz_vars)

#using altair syntax
wz_data_types = "Nominal", "Quantitative", "Quantitative", "Nominal", "Nominal", "Nominal", "Nominal", "Nominal", "Nominal", "Quantitative", "Quantitative", "Ordinal", "N/A", "N/A", "N/A"

```

We can create a table of variable names and data types below:
```{python}
vars_and_types = pd.DataFrame({
  'Variable Names': wz_vars,
  'Variable Types': wz_data_types
})

print(vars_and_types)
```

2. 

```{python}

wz_full = pd.read_csv("waze_data.csv", index_col=None)

wz_full.head()

#Create a stacked bar chart where x axis is each variable and it shows proportion of missing vs. non missing values
#To do this, we need to create a function and then apply() it to each variable in the df

def calc_missing(column):
    missing_no = column.isna().sum()
    non_missing_no = len(column) - missing_no
    return pd.Series({'Missing': missing_no, 'Non-missing': non_missing_no})

missing_counts = wz_full.apply(calc_missing)
print(missing_counts)

#This needs to be converted from wide to long so that each variable (city, confidence, etc.) is in one col. We can do this using melt

missing_counts_long = missing_counts.reset_index().melt(
    id_vars='index',
    var_name='Variable Name',
    value_name='Count of Observations'
)
print(missing_counts_long)


wz_missing_stacked = alt.Chart(missing_counts_long).mark_bar().encode(
  x='Variable Name',
  y='Count of Observations',
  color='index'
)

wz_missing_stacked

```
We can see that nThumbsUp, street, and subtype have missing values, with nThumbsUp having the largest proportion of missing values.

3. Print the unique values for columns type and subtype:

```{python}
#We need to group the data by type, and then list the subtypes for each type

wz_group_by_type = wz_full.groupby('type')
pd.set_option('display.max_colwidth', None)

full_subtypes = (wz_group_by_type['subtype'].unique())
print(full_subtypes)

subtypes_list = wz_full['subtype'].unique()
print(subtypes_list)

```

We can see that all four types (ACCIDENT, HAZARD, JAM, ROAD_CLOSED) have the NA subtype. We can also see that the HAZARD type has enough similar subtypes to consider the possibility of sub-subtypes (e.g. HAZARD_ON_ROAD_CAR_STOPPED, HAZARD_ON_ROAD_CONSTRUCTION, HAZARD_ON_ROAD_EMERGENCY_VEHICLE, etc. could have a HAZARD_ON_ROAD sub-subtype)

Bulleted Hierarchy:
-Accident
    -Major
    -Minor
    -NA (Unclassified)
-Hazard
    -On Road
        -Stopped Car
        -Construction
        -Emergency Vehicle
        -Ice
        -Object
        -Pot Hole
        -Traffic Light Fault
        -Lane Closed
        -Roadkill
    -On Shoulder
        -Stopped Car
        -Animals
        -Missing Sign
    -Weather
        -Floor
        -Fog
        -Heavy Snow
        -Hail
    -NA (Unclassified)
-Jam
    -Standstill Traffic
    -Heavy Traffic
    -Moderate Traffic
    -Light Traffic
    -NA (Unclassified)
-Road Closed
    -Event
    -Construction
    -Hazard
    -NA (Unclassified)

Should we keep the NA subtypes or not?

To answer this let's run a count of NA values by type:
```{python}
na_counts = wz_full.groupby('type')['subtype'].apply(lambda incident_type: incident_type.isna().sum())
print(na_counts)

```
Over 90,000 (12%) of our observations have a NA subtype, which represent a significant slice of our data. We can also see that some accident types are disproportionately affected by NAs more than others; over half of NA values are JAM-type incidents (which makes sense, since heavy, moderate, and light traffic is very subjective). Simply moving NAs could thus potentially introduce bias.

4. 
4a. Define a crosswalk df:
```{python}
col_names = ["type", "subtype", "updated_type", "updated_subtype", "updated_subsubtype"]
crosswalk = pd.DataFrame(columns=col_names)

```

4b. Fill in the crosswalk with all unique combinations of type/subtype, then fill in updated_type, updated_subtype, and updated_subsubtype accordingly

(We can do this using drop_duplicates)
```{python}

wz_full = wz_full.fillna("Unclassified")

unique_combinations = wz_full[['type', 'subtype']].drop_duplicates()
crosswalk[['type', 'subtype']] = unique_combinations.reset_index(drop=True)

#We can create a mapping function to copy over the types into updated_type using a dictionary:

def modify_type(value):
    replacements = {
    "JAM": "Jam",
    "ACCIDENT": "Accident",
    "ROAD_CLOSED": "Road Closed",
    "HAZARD": "Hazard"
    }
    return replacements.get(value)

def modify_subtype(value):
    if "HAZARD_ON_ROAD" in value:
        return "Hazard - Road"
    elif "HAZARD_ON_SHOULDER" in value:
        return "Hazard - Shoulder"
    elif "HAZARD_ON_WEATHER" in value:
        return "Hazard - Weather"
    else:
        return value

def modify_subsubtype(value):
    replacements = {
    "Unclassified": "Unclassified",
    "ACCIDENT_MAJOR": "Major Accident",
    "ACCIDENT_MINOR": "Minor Accident",   
    "HAZARD_ON_ROAD": "Road Hazard",
    "HAZARD_ON_ROAD_CAR_STOPPED": "Stopped Car (Road)",
    "HAZARD_ON_ROAD_CONSTRUCTION": "Construction",
    "HAZARD_ON_ROAD_EMERGENCY_VEHICLE": "Emergency Vehicle",
    "HAZARD_ON_ROAD_ICE": "Ice",
    "HAZARD_ON_ROAD_OBJECT": "Object on Road",
    "HAZARD_ON_ROAD_POT_HOLE": "Pot Hole",
    "HAZARD_ON_ROAD_TRAFFIC_LIGHT_FAULT": "Traffic Light Fault",
    "HAZARD_ON_ROAD_LANE_CLOSED": "Lane Closure",
    "HAZARD_ON_ROAD_ROAD_KILL": "Roadkill",
    "HAZARD_ON_SHOULDER": "Shoulder Hazard",
    "HAZARD_ON_SHOULDER_CAR_STOPPED": "Stopped Car (Shoulder)",
    "HAZARD_ON_SHOULDER_ANIMALS": "Animals",
    "HAZARD_ON_SHOULDER_MISSING_SIGN": "Missing Sign",
    "HAZARD_WEATHER": "Weather",
    "HAZARD_WEATHER_FLOOD": "Flood",
    "HAZARD_WEATHER_HAIL": "Hail",
    "HAZARD_WEATHER_FOG": "Fog",
    "HAZARD_WEATHER_HEAVY_SNOW": "Heavy Snow",
    "JAM_HEAVY_TRAFFIC": "Heavy Traffic",
    "JAM_MODERATE_TRAFFIC": "Moderate Traffic",
    "JAM_STAND_STILL_TRAFFIC": "Standstill Traffic",
    "JAM_LIGHT_TRAFFIC": "Light Traffic",
    "ROAD_CLOSED_EVENT": "Event",
    "ROAD_CLOSED_HAZARD": "Hazard",
    "ROAD_CLOSED_CONSTRUCTION": "Construction",
    }
    return replacements.get(value)

crosswalk['updated_type'] = crosswalk['type'].apply(modify_type)
crosswalk['updated_subtype'] = crosswalk['subtype'].apply(modify_subtype)
crosswalk['updated_subsubtype'] = crosswalk['subtype'].apply(modify_subsubtype)

#This code will clean up the lingering old variable names in updated_subtype 

def transpose_subtype(subtype, subsubtype):
    if "_" in subtype:
        return subsubtype
    else:
        return subtype

crosswalk['updated_subtype'] = crosswalk.apply(lambda row: transpose_subtype(row['updated_subtype'], row['updated_subsubtype']), axis=1)

#This code removes duplicate subsubtypes where they're not needed or where subsubtypes don't exist bc a subtype is sufficient
def remove_subsubtypes(subtype, subsubtype):
    if subtype == subsubtype:
        return "Unclassified"
    else:
        return subsubtype

crosswalk['updated_subsubtype'] = crosswalk.apply(lambda row: remove_subsubtypes(row['updated_subtype'], row['updated_subsubtype']), axis=1)

```
4c. Merge the crosswalk with the original data set on type and subtype. How many rows are there for Accident - Unclassified?
 
```{python}
print(crosswalk.columns)
print(wz_full.columns)

wz_merge = pd.merge(crosswalk, wz_full, on=['type', 'subtype'], how='inner')

wz_filter = wz_merge[(wz_merge["updated_type"]  == "Accident") & (wz_merge["updated_subtype"] == "Unclassified")]

print(f"There are {len(wz_filter)} rows for Accident - Unclassified type incidents.")


```
Extra Credit: 
```{python}
#Extra credit: Use drop_duplicates() to find the same combinations of values in type and subtype for crosswalk and the merged dataset

set_cw = (crosswalk[['updated_type', 'updated_subtype']].drop_duplicates()).reset_index(drop=True)
set_wzm = (wz_merge[['updated_type', 'updated_subtype']].drop_duplicates()).reset_index(drop=True)

#This results in two dfs where each row is a combination of unique updated_type and updated_subtype
#Attribution: Asked ChatGPT how to compare these two df's combinations of type and subtype regardless of order- my previous code compared the columns directly using a pairwise comparison
# Tuples have no order, so using itertuples lets us compare the combinations of type and subtype regardless of order

set_cw_tuples = set(set_cw.itertuples(index=False, name=None))
set_wzm_tuples = set(set_wzm.itertuples(index=False, name=None))

print(set_cw_tuples == set_wzm_tuples)


```



# App #1: Top Location by Alert Type Dashboard (30 points){-}

1. 


```{python}
#placeholder app
from shiny import App, render, ui

app_ui = ui.page_fluid(
    #placeholder code
)

def server(input, output, session):
    #placeholder python

app = App(app_ui, server)

```

a. Use regex to extract latitude/longitude from the geo variable in the data
```{python}
#Attribution: Used ChatGPT by prompting "How to extract latitude and longitude from the geoWKT variable using regex" with a screenshot of the data. Response was to use the following code (substituting df for wz_merge:)

wz_merge[['longitude', 'latitude']] = wz_merge['geoWKT'].str.extract(r'Point\((-?\d+\.\d+)\s(-?\d+\.\d+)\)')

```

b. Put the latitude/longitude data into bins. Which long/lat combination has the greatest number of observations?
```{python}
wz_merge['longitude'] = pd.to_numeric(wz_merge['longitude']).round(2)
wz_merge['latitude'] = pd.to_numeric(wz_merge['latitude']).round(2)

wz_grouped = wz_merge.groupby(['longitude', 'latitude']).size().reset_index(name="count")

wz_grouped.sort_values(by="count", ascending=False).head()

```
We can see that the combination (41.88, -87.65) has the greatest number of observations (21,325 rows).

c. 
```{python}
wz_by_type = wz_merge.groupby(['longitude', 'latitude', 'updated_type', 'updated_subtype']).size().reset_index(name="count")

top_wz = wz_by_type.sort_values(by="count", ascending=False)

top_wz.to_csv('top_alerts_map/top_alerts_map.csv', index=False)

print(f"This dataset has {len(top_wz)} rows.")

```

Each row in this dataframe (top_10_wz) should correspond to a 4-way combination of longitude, latitude, type, and subtype, with the number of alerts for that particular combination.


2. 

Using altair, plot a scatterplot of the top 10 latitude-longitude bins for "Jam - Heavy Traffic Alerts"

```{python}

top_10_wz = top_wz[(top_wz["updated_type"] == "Jam") & (top_wz["updated_subtype"] == "Heavy Traffic")].sort_values(by="count", ascending=False).head(10)

jam_plot = alt.Chart(top_10_wz).mark_circle().encode(
    x=alt.X("longitude:Q", scale=alt.Scale(domain=[-87.8, -87.6])),
    y=alt.Y("latitude:Q", scale=alt.Scale(domain=[41.8, 42.1])),
    size=alt.Size("count:Q", title="Count of Alerts"),
    color=alt.Color("count:Q"),
    tooltip=["longitude", "latitude", "count"]
)

jam_plot

```

3. Next, we will layer this scatterplot on a map of Chicago:
    
a. Download the neighborhood boundaries as a GeoJSON:

```{python}
print(os.getcwd())
```

```{python}
#Attribution: Asked ChatGPT to explain the requests package in python
import requests

url = "https://data.cityofchicago.org/api/geospatial/bbvz-uum9?method=export&format=GeoJSON"
filepath = "neighborhood_boundaries.geojson"

response = requests.get(url)
#this creates a reponse object with a status_code attribute
if response.status_code == 200:
    with open(filepath, "wb") as file:
        file.write(response.content)    
else:
    print("Error: HTTP Status Code {response.status_code}")


```
    

b. 
```{python}
# MODIFY ACCORDINGLY

import geopandas as gpd

file_path = "/Users/charleshuang/Documents/GitHub/real_merge_example/ppha30538-problem-set-6/neighborhood_boundaries.geojson"
#----

with open(file_path) as f:
    chicago_geojson = json.load(f)

geo_data = alt.Data(values=chicago_geojson["features"])

```



4. Layer the scatterplot from the previous step onto the new map


```{python}

chicago_map = alt.Chart(geo_data).mark_circle().encode(
    longitude='longitude:Q',
    latitude='latitude:Q',
    size=alt.value(10),
    tooltip=["pri_neigh:N", "sec_neigh:N"]
).project(
    "equirectangular"
)
chicago_map

```

```{python}
#Test code
import geopandas as gpd
import pandas as pd
import json

# Load GeoJSON file
file_path = "/Users/charleshuang/Documents/GitHub/real_merge_example/ppha30538-problem-set-6/neighborhood_boundaries.geojson"
with open(file_path) as f:
    chicago_geojson = json.load(f)

# Convert GeoJSON to GeoDataFrame
gdf = gpd.GeoDataFrame.from_features(chicago_geojson["features"])

# Set the CRS (assuming WGS84 - EPSG 4326)
gdf.set_crs(epsg=4326, inplace=True)

# Reproject if needed (in this case, not necessary since it's already WGS84)
gdf = gdf.to_crs(epsg=4326)

# Extract centroids
gdf["longitude"] = gdf.geometry.centroid.x
gdf["latitude"] = gdf.geometry.centroid.y

# Flatten to a DataFrame for Altair
df_chicago = pd.DataFrame(gdf)


chicago_map = alt.Chart(alt.Data(values=chicago_geojson["features"])).mark_geoshape().encode(
    tooltip=["properties.pri_neigh:N", "properties.sec_neigh:N"]
).project(
    type="equirectangular"
).properties(
    width=400,
    height=300,
    title="Chicago Neighborhoods"
)

chicago_map






```

5. 

a. 

```{python}

```

b. 
```{python}

```

c. 
```{python}

```

d. 
```{python}

```

e. 

# App #2: Top Location by Alert Type and Hour Dashboard (20 points) {-}

1. 

a. 


    
b. 
```{python}

```

c.

```{python}

```
    

2.

a. 



b. 


c. 


# App #3: Top Location by Alert Type and Hour Dashboard (20 points){-}

1. 


a. 

b. 

```{python}

```

2. 

a. 


b. 
    
3. 

a. 
    

b. 


c. 


d.
